
<!DOCTYPE html>
<html>

  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta http-equiv="X-UA-Compatible" content="IE=edge">

<title>Ruiqi Gao | publications</title>
<meta name="description" content="Ruiqi Gao's academic website.
">

<!-- Open Graph -->


<!-- Bootstrap & MDB -->
<link href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css" rel="stylesheet" integrity="sha512-MoRNloxbStBcD8z3M/2BmnT+rg4IsMxPkXaGh2zD6LGNNFE80W3onsAhRcMAMrSoyWL9xD7Ert0men7vR8LUZg==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/css/mdb.min.css" integrity="sha512-RO38pBRxYH3SoOprtPTD86JFOclM51/XTIdEPh5j8sj4tp8jmQIx26twG52UaLi//hQldfrh7e51WzP9wuP32Q==" crossorigin="anonymous" />

<!-- Fonts & Icons -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css"  integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.0/css/academicons.min.css" integrity="sha512-W4yqoT1+8NLkinBLBZko+dFB2ZbHsYLDdr50VElllRcNt2Q4/GSs6u71UHKxB7S6JEMCp5Ve4xjh3eGQl/HRvg==" crossorigin="anonymous">
<link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

<!-- Code Syntax Highlighting -->
<link rel="stylesheet" href="https://gitcdn.link/repo/jwarby/jekyll-pygments-themes/master/github.css" />

<!-- Styles -->
<link rel="shortcut icon" href="/assets/img/favicon.ico">
<link rel="stylesheet" href="main.css">

<link rel="canonical" href="publication">

<!-- Theming-->


    
<!-- MathJax -->
<script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.1.2/es5/tex-mml-chtml.js"></script>
<script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>


  </head>

  <body class="fixed-top-nav sticky-bottom-footer">

    <!-- Header -->

    <header>

    <!-- Nav Bar -->
    <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
    <div class="container">
      
      <a class="navbar-brand title font-weight-lighter" href="/">
       <span class="font-weight-bold">Ruiqi Gao</span>
      </a>
      
      <!-- Navbar Toogle -->
      <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar top-bar"></span>
        <span class="icon-bar middle-bar"></span>
        <span class="icon-bar bottom-bar"></span>
      </button>
      <div class="collapse navbar-collapse text-right" id="navbarNav">
        <ul class="navbar-nav ml-auto flex-nowrap">
          <!-- About -->
          <li class="nav-item ">
            <a class="nav-link" href="index.html">
              About
              
            </a>
          </li>
          
          <!-- Blog -->
          <!--  <li class="nav-item ">
            <a class="nav-link" href="/blog/">
              blog
              
            </a>
          </li>
           -->
          <!-- Other pages -->
          
          
          
          <li class="nav-item active">
              <a class="nav-link" href="publications.html">
                Publications
                
                <span class="sr-only">(current)</span>
                
              </a>
          </li>

          <li class="nav-item ">
              <a class="nav-link" href="CV.pdf">
                CV                
              </a>
          </li>
          
          
          
        </ul>
      </div>
    </div>
  </nav>

</header>


    <!-- Content -->

    <div class="container mt-5">
      <div class="post">

  <header class="post-header">
    <h1 class="post-title">Publications</h1>
    <p class="post-description">(&#x2A) denotes equal contribution.</p>
  </header>

  <article>
    <!-- An up-to-date list is available on <a href="https://scholar.google.com/citations?user=VdlgOXoAAAAJ&hl=en" target="\_blank">Google Scholar</a> -->
<div class="publications">

  <h2 class="year">2021</h2>
  <ol class="bibliography">

 <li><div class="row">
  <div class="col-sm-2 abbr">
    <abbr class="badge">AAAI</abbr>
  </div>

  <div id="gao2020v1" class="col-sm-8">
    
      <div class="title">Learning Vector Representation of Local Content and Matrix Representation of Local Motion, with Implications for V1</div>
      <div class="author">
                <em>Ruiqi Gao</em>, Jianwen Xie, Siyuan Huang, Yufan Ren, <a href="http://www.stat.ucla.edu/~sczhu/">Song-Chun Zhu</a>, and <a href="http://www.stat.ucla.edu/~ywu/" target="_blank">Ying Nian Wu</a>
        
      </div>
   <div class="periodical">    
        <em>The Thirty-Sixth AAAI Conference on Artificial Intelligence (AAAI)</em>, 2022
      </div>  

    <div class="links">
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
      <a href="https://arxiv.org/pdf/1902.03871.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
    </div>

    <!-- Hidden abstract block -->
    <div class="abstract hidden">
      <p>This paper proposes a representational model for image pair such as consecutive video frames that are related by local pixel displacements, in the hope that the model may shed light on motion perception in primary visual cortex (V1). The model couples the following two components. (1) The vector representations of local contents of images. (2) The matrix representations of local pixel displacements caused by the relative motions between the agent and the objects in the 3D scene. When the image frame undergoes changes due to local pixel displacements, the vectors are multiplied by the matrices that represent the local displacements. Our experiments show that our model can learn to infer local motions. Moreover, the model can learn Gabor-like filter pairs of quadrature phases.</p>
    </div>
  </div>
</div>
</li>


 <li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">NeurIPS</abbr>
  </div>

  <div id="gao2021grid" class="col-sm-8">
    
      <div class="title">On Path Integration of Grid Cells: Group Representation and Isotropic Scaling</div>
      <div class="author">
                <em>Ruiqi Gao</em>, Jianwen Xie, <a href="https://sites.google.com/view/xxweineuraltheory/home">Xue-Xin Wei</a>, <a href="http://www.stat.ucla.edu/~sczhu/">Song-Chun Zhu</a>, and <a href="http://www.stat.ucla.edu/~ywu/" target="_blank">Ying Nian Wu</a>
        
      </div>
   <div class="periodical">
      
        <em>The 35th Conference on Neural Information Processing Systems</em>, 2021
  
      </div>  

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
      <a href="https://arxiv.org/pdf/2006.10259.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      <a href="https://github.com/ruiqigao/grid-cell-path" class="btn btn-sm z-depth-0" role="button" target="_blank">Code</a>
    </div>

    <!-- Hidden abstract block -->
    <div class="abstract hidden">
      <p>Understanding how grid cells perform path integration calculations remains a fundamental problem. In this paper, we conduct theoretical analysis of a general representation model of path integration by grid cells, where the 2D self-position is encoded as a higher dimensional vector, and the 2D self-motion is represented by a general transformation of the vector. We identify two conditions on the transformation. One is a group representation condition that is necessary for path integration. The other is an isotropic scaling condition that ensures locally conformal embedding, so that the error in the vector representation translates conformally to the error in the 2D self-position. Then we investigate the simplest transformation, i.e., the linear transformation, uncover its explicit algebraic and geometric structure as matrix Lie group of rotation, and explore the connection between the isotropic scaling condition and a special class of hexagon grid patterns. Finally, with our optimization-based approach, we manage to learn hexagon grid patterns that share similar properties of the grid cells in the rodent brain. The learned model is capable of accurate long distance path integration.</p>
    </div>
    
  </div>
</div>
</li>


<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">CVPR</abbr>
  </div>

  <div id="zhu2021learning" class="col-sm-8">
    
      <div class="title">Learning Neural Representation of Camera Pose with Matrix Representation of Pose Shift via View Synthesis</div>
      <div class="author">
                Yaxuan Zhu, <em>Ruiqi Gao</em>, Siyuan Huang, <a href="http://www.stat.ucla.edu/~sczhu/">Song-Chun Zhu</a>, and <a href="http://www.stat.ucla.edu/~ywu/" target="_blank">Ying Nian Wu</a>
        
      </div>
   <div class="periodical">
      
        <em>Conference on Computer Vision and Pattern Recognition (CVPR)</em>,
        2021
  
      </div>  

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
      <a href="https://arxiv.org/pdf/2104.01508.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      <a href="https://github.com/AlvinZhuyx/camera_pose_representation" class="btn btn-sm z-depth-0" role="button" target="_blank">Code</a>
    </div>

    <!-- Hidden abstract block -->
    <div class="abstract hidden">
      <p>How to effectively represent camera pose is an essential problem in 3D computer vision, especially in tasks such as camera pose regression and novel view synthesis. Traditionally, 3D position of the camera is represented by Cartesian coordinate and the orientation is represented by Euler angle or quaternions. These representations are manually designed, which may not be the most effective representation for downstream tasks. In this work, we propose an approach to learn neural representations of camera poses and 3D scenes, coupled with neural representations of local camera movements. Specifically, the camera pose and 3D scene are represented as vectors and the local camera movement is represented as a matrix operating on the vector of the camera pose. We demonstrate that the camera movement can further be parametrized by a matrix Lie algebra that underlies a rotation system in the neural space. The vector representations are then concatenated and generate the posed 2D image through a decoder network. The model is learned from only posed 2D images and corresponding camera poses, without access to depths or shapes. We conduct extensive experiments on synthetic and real datasets. The results show that compared with other camera pose representations, our learned representation is more robust to noise in novel view synthesis and more effective in camera pose regression.</p>
    </div>
    
  </div>
</div>
</li>


    <li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">ICLR</abbr>
  </div>

  <div id="gao2020learning" class="col-sm-8">
    
      <div class="title">Learning Energy­-Based Models by Diffusion Recovery Likelihood</div>
      <div class="author">
                <em>Ruiqi Gao</em>, Yang Song, <a href="https://cs.stanford.edu/~poole/">Ben Poole</a>, <a href="http://www.stat.ucla.edu/~ywu/" target="_blank">Ying Nian Wu</a>, and <a href="http://dpkingma.com/" target="_blank"> Diederik P. Kingma</a>
        
      </div>
    <div class="periodical">
      
        <em>Ninth International Conference of Learning Representations (ICLR)</em>,
        2021
  
      </div> 

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
      <a href="https://arxiv.org/pdf/2012.08125.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      <a href="https://github.com/ruiqigao/recovery_likelihood" class="btn btn-sm z-depth-0" role="button" target="_blank">Code</a>
    </div>

    <!-- Hidden abstract block -->
    <div class="abstract hidden">
      <p>While energy-based models (EBMs) exhibit a number of desirable properties, training and sampling on high-dimensional datasets remains challenging. Inspired by recent progress on diffusion probabilistic models, we present a diffusion recovery likelihood method to tractably learn and sample from a sequence of EBMs trained on increasingly noisy versions of a dataset. Each EBM is trained with recovery likelihood,  which maximizes the conditional probability of the data at a certain noise level given their noisy versions at a higher noise level. Optimizing recovery likelihood is more tractable than marginal likelihood, as sampling from the conditional distributions is much easier than sampling from the marginal distributions. After training, synthesized images can be generated by the sampling process that initializes from Gaussian white noise distribution and progressively samples the conditional distributions at decreasingly lower noise levels.  Our method generates high fidelity samples on various image datasets. On unconditional CIFAR-10 our method achieves FID 9.58 and inception score 8.30, superior to the majority of GANs. Moreover, we demonstrate that unlike previous work on EBMs, our long-run MCMC samples from the conditional distributions do not diverge and still represent realistic images, allowing us to accurately estimate the normalized density of data even for high-dimensional datasets.</p>
    </div>
    
  </div>
</div>
</li>


  <h2 class="year">2020</h2>
  <ol class="bibliography">

 <li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">arXiv</abbr>
  </div>

  <div id="nijkamp2020learning" class="col-sm-8">
    
      <div class="title">Learning Energy-based Model with Flow-Based Backbone by Neural Transport MCMC</div>
      <div class="author">
                Erik Nijkamp*, <em>Ruiqi Gao*</em>, Pavel Sountsov, Srinivas Vasudevan, Bo Pang, <a href="http://www.stat.ucla.edu/~sczhu/">Song-Chun Zhu</a>, and <a href="http://www.stat.ucla.edu/~ywu/" target="_blank">Ying Nian Wu</a>
        
      </div>
   <div class="periodical">
      
        <em>arXiv preprint: 2006.06897</em>,
        2020
      
      </div>  

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
      <a href="https://arxiv.org/pdf/2006.06897.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
    </div>

    <!-- Hidden abstract block -->
    <div class="abstract hidden">
      <p>Learning energy-based model (EBM) requires MCMC sampling of the learned model as the inner loop of the learning algorithm. However, MCMC sampling of EBM in data space is generally not mixing, because the energy function, which is usually parametrized by deep network, is highly multi-modal in the data space. This is a serious handicap for both the theory and practice of EBM. In this paper, we propose to learn EBM with a flow-based model serving as a backbone, so that the EBM is a correction or an exponential tilting of the flow-based model. We show that the model has a particularly simple form in the space of the latent variables of the flow-based model, and MCMC sampling of the EBM in the latent space, which is a simple special case of neural transport MCMC, mixes well and traverses modes in the data space. This enables proper sampling and learning of EBM.</p>
    </div>
    
  </div>
</div>
</li>

 <li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">CVPR</abbr>
    <span class="award badge">Oral</span>
  </div>

  <div id="gao2020flow" class="col-sm-8">
    
      <div class="title">Flow Contrastive Estimation of Energy­-Based Models</div>
      <div class="author">
                <em>Ruiqi Gao</em>, Erik Nijkamp, <a href="http://dpkingma.com/" target="_blank"> Diederik P. Kingma</a>, Zhen Xu, Andrew M. Dai, and <a href="http://www.stat.ucla.edu/~ywu/" target="_blank">Ying Nian Wu</a>
        
      </div>
   <div class="periodical">    
        <em>Conference on Computer Vision and Pattern Recognition (CVPR)</em>,
        2020
      </div>  
       <div class="periodical">    
        <em>NeurIPS workshop on Bayesian Deep Learning</em>, 2019
      </div>  

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
      <a href="https://arxiv.org/pdf/1912.00589.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      <a href="http://www.stat.ucla.edu/~ruiqigao/fce/main.html" class="btn btn-sm z-depth-0" role="button" target="_blank">Website</a>
    </div>

    <!-- Hidden abstract block -->
    <div class="abstract hidden">
      <p>This paper studies a training method to jointly estimate an energy-based model and a flow-based model, in which the two models are iteratively updated based on a shared adversarial value function. This joint training method has the following traits. (1) The update of the energy-based model is based on noise contrastive estimation, with the flow model serving as a strong noise distribution. (2) The update of the flow model approximately minimizes the Jensen-Shannon divergence between the flow model and the data distribution. (3) Unlike generative adversarial networks (GAN) which estimates an implicit probability distribution defined by a generator model, our method estimates two explicit probabilistic distributions on the data. Using the proposed method we demonstrate a significant improvement on the synthesis quality of the flow model, and show the effectiveness of unsupervised feature learning by the learned energy-based model. Furthermore, the proposed training method can be easily adapted to semi-supervised learning. We achieve competitive results to the state-of-the-art semi-supervised learning methods.</p>
    </div>
  </div>
</div>
</li>

 <li><div class="row">
  <div class="col-sm-2 abbr">
    <abbr class="badge">AAAI</abbr>
    <span class="award badge">Oral</span>
  </div>

  <div id="xie2020motion" class="col-sm-8">
    
      <div class="title">Motion-Based Generator Model: Unsupervised Disentanglement of Appearance, Trackable and Intrackable Motions in Dynamic Patterns</div>
      <div class="author">
                Jianwen Xie*, <em>Ruiqi Gao*</em>, Zilong Zheng, <a href="http://www.stat.ucla.edu/~sczhu/">Song-Chun Zhu</a>, and <a href="http://www.stat.ucla.edu/~ywu/" target="_blank">Ying Nian Wu</a>
        
      </div>
   <div class="periodical">    
        <em>The Thirty-Fourth AAAI Conference on Artificial Intelligence (AAAI)</em>, 2020
      </div>  

    <div class="links">
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
      <a href="http://www.stat.ucla.edu/~jxie/MotionBasedGenerator/MotionBasedGenerator_file/doc/MotionBasedGenerator.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      <a href="http://www.stat.ucla.edu/~jxie/MotionBasedGenerator/MotionBasedGenerator.html" class="btn btn-sm z-depth-0" role="button" target="_blank">Website</a>
    </div>

    <!-- Hidden abstract block -->
    <div class="abstract hidden">
      <p>Dynamic patterns are characterized by complex spatial and motion patterns. Understanding dynamic patterns requires a disentangled representational model that separates the factorial components. A commonly used model for dynamic patterns is the state space model, where the state evolves over time according to a transition model and the state generates the observed image frames according to an emission model. To model the motions explicitly, it is natural for the model to be based on the motions or the displacement fields of the pixels. Thus in the emission model, we let the hidden state generate the displacement field, which warps the trackable component in the previous image frame to generate the next frame while adding a simultaneously emitted residual image to account for the change that cannot be explained by the deformation. The warping of the previous image is about the trackable part of the change of image frame, while the residual image is about the intrackable part of the image. We use a maximum likelihood algorithm to learn the model parameters that iterates between inferring latent noise vectors that drive the transition model and updating the parameters given the inferred latent vectors. Meanwhile we adopt a regularization term to penalize the norms of the residual images to encourage the model to explain the change of image frames by trackable motion. Unlike existing methods on dynamic patterns, we learn our model in unsupervised setting without ground truth displacement fields or optical flows. In addition, our model defines a notion of intrackability by the separation of warped component and residual component in each image frame. We show that our method can synthesize realistic dynamic pattern, and disentangling appearance, trackable and intrackable motions. The learned models can be useful for motion transfer, and it is natural to adopt it to define and measure intrackability of a dynamic pattern.</p>
    </div>
  </div>
</div>
</li>
</ol>


<h2 class="year">2019</h2>
<ol class="bibliography">

 <li><div class="row">
  <div class="col-sm-2 abbr">
    <abbr class="badge">Journal</abbr>
  </div>

  <div id="xie2019representation" class="col-sm-8">    
      <div class="title">Representation Learning: A Statistical Perspective</div>
      <div class="author">
                Jianwen Xie, <em>Ruiqi Gao</em>, Erik Nijkamp, <a href="http://www.stat.ucla.edu/~sczhu/">Song-Chun Zhu</a>, and <a href="http://www.stat.ucla.edu/~ywu/" target="_blank">Ying Nian Wu</a>
        
      </div>
   <div class="periodical">    
        <em>Annual Review of Statistics and Its Application (ARSIA)</em>, 2020
      </div>  

    <div class="links">
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
      <a href="http://www.stat.ucla.edu/~jxie/personalpage_file/publications/representation_learning_Review.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
    </div>

    <!-- Hidden abstract block -->
    <div class="abstract hidden">
      <p>Learning representations of data is an important problem in statistics and machine learning. While the origin of learning representations can be traced back to factor analysis and multidimensional scaling in statistics, it has become a central theme in deep learning with important applications in computer vision and computational neuroscience. In this article, we review recent advances in learning representations from a statistical perspective. In particular, we review the following two themes: (a) unsupervised learning of vector representations and (b) learning of both vector and matrix representations.</p>
    </div>
  </div>
</div>
</li>


 <li><div class="row">
  <div class="col-sm-2 abbr">
    <abbr class="badge">ICLR</abbr>
  </div>

  <div id="gao2019learning" class="col-sm-8">    
      <div class="title">Learning Grid Cells as Vector Representation of Self­-Position Coupled with Matrix Representation of Self­-Motion</div>
      <div class="author">
                <em>Ruiqi Gao*</em>, Jianwen Xie*, <a href="http://www.stat.ucla.edu/~sczhu/">Song-Chun Zhu</a>, and <a href="http://www.stat.ucla.edu/~ywu/" target="_blank">Ying Nian Wu</a>
        
      </div>
   <div class="periodical">    
        <em>Seventh International Conference on Learning Representations (ICLR)</em>, 2019
      </div>  

    <div class="links">
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
      <a href="https://openreview.net/pdf?id=Syx0Mh05YQ" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      <a href="http://www.stat.ucla.edu/~ruiqigao/gridcell/main.html" class="btn btn-sm z-depth-0" role="button" target="_blank">Website</a>
      <a href="https://github.com/ruiqigao/GridCell" class="btn btn-sm z-depth-0" role="button" target="_blank">Code</a>
    </div>

    <!-- Hidden abstract block -->
    <div class="abstract hidden">
      <p>This paper proposes a representational model for grid cells. In this model, the 2D self-position of the agent is represented by a high-dimensional vector, and the 2D self-motion or displacement of the agent is represented by a matrix that transforms the vector. Each component of the vector is a unit or a cell. The model consists of the following three sub-models. (1) Vector-matrix multiplication. The movement from the current position to the next position is modeled by matrix-vector multiplication, i.e., the vector of the next position is obtained by multiplying the matrix of the motion to the vector of the current position. (2) Magnified local isometry. The angle between two nearby vectors equals the Euclidean distance between the two corresponding positions multiplied by a magnifying factor. (3) Global adjacency kernel. The inner product between two vectors measures the adjacency between the two corresponding positions, which is defined by a kernel function of the Euclidean distance between the two positions. Our representational model has explicit algebra and geometry. It can learn hexagon patterns of grid cells, and it is capable of error correction, path integral and path planning.</p>
    </div>
  </div>
</div>
</li>


 <li><div class="row">
  <div class="col-sm-2 abbr">
    <abbr class="badge">AAAI</abbr>
  </div>

  <div id="xie2019learning2" class="col-sm-8">    
      <div class="title">Learning Dynamic Generator Model by Alternating Back­-Propagation Through Time</div>
      <div class="author">
                Jianwen Xie*, <em>Ruiqi Gao*</em>, Zilong Zheng, <a href="http://www.stat.ucla.edu/~sczhu/">Song-Chun Zhu</a>, and <a href="http://www.stat.ucla.edu/~ywu/" target="_blank">Ying Nian Wu</a>
        
      </div>
   <div class="periodical">    
        <em>The Thirty­-Third AAAI Conference on Artificial Intelligence (AAAI)</em>, 2019
      </div>  

    <div class="links">
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
      <a href="http://www.stat.ucla.edu/~jxie/DynamicGenerator/DynamicGenerator_file/doc/DynamicGenerator.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      <a href="http://www.stat.ucla.edu/~jxie/DynamicGenerator/DynamicGenerator.html" class="btn btn-sm z-depth-0" role="button" target="_blank">Website</a>
      <a href="https://github.com/jianwen-xie/Dynamic_generator" class="btn btn-sm z-depth-0" role="button" target="_blank">Code</a>
    </div>

    <!-- Hidden abstract block -->
    <div class="abstract hidden">
      <p>This paper studies the dynamic generator model for spatial-temporal processes such as dynamic textures and action sequences in video data. In this model, each time frame of the video sequence is generated by a generator model, which is a non-linear transformation of a latent state vector, where the non-linear transformation is parametrized by a top-down neural network. The sequence of latent state vectors follows a non-linear auto-regressive model, where the state vector of the next frame is a non-linear transformation of the state vector of the current frame as well as an independent noise vector that provides randomness in the transition. The non-linear transformation of this transition model can be parametrized by a feedforward neural network. We show that this model can be learned by an alternating back-propagation through time algorithm that iteratively samples the noise vectors and updates the parameters in the transition model and the generator model. We show that our training method can learn realistic models for dynamic textures and action patterns.</p>
    </div>
  </div>
</div>
</li>


<li><div class="row">
  <div class="col-sm-2 abbr">
    <abbr class="badge">CVPR</abbr>
    <abbr class="badge">TPAMI</abbr>
  </div>

  <div id="xing2020deformable" class="col-sm-8">
    
      <div class="title">Deformable Generator Networks: Unsupervised Disentanglement of Appearance and Geometry</div>
      <div class="author">
                Xianglei Xing, <em>Ruiqi Gao</em>, Tian Han, <a href="http://www.stat.ucla.edu/~sczhu/">Song-Chun Zhu</a>, and <a href="http://www.stat.ucla.edu/~ywu/" target="_blank">Ying Nian Wu</a>
        
      </div>
          <div class="periodical">    
        <em>Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2019
      </div> 
   <div class="periodical">    
        <em>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</em>, 2020
      </div>  


    <div class="links">
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
      <a href="https://arxiv.org/pdf/1806.06298.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      <a href="https://andyxingxl.github.io/Deformable-generator/" class="btn btn-sm z-depth-0" role="button" target="_blank">Website</a>
      <a href="https://github.com/andyxingxl/Deformable-generator" class="btn btn-sm z-depth-0" role="button" target="_blank">Code</a>
    </div>

    <!-- Hidden abstract block -->
    <div class="abstract hidden">
      <p>We present a deformable generator model to disentangle the appearance and geometric information for both image and video data in a purely unsupervised manner. The appearance generator network models the information related to appearance, including color, illumination, identity or category, while the geometric generator performs geometric warping, such as rotation and stretching, through generating deformation field which is used to warp the generated appearance to obtain the final image or video sequences. Two generators take independent latent vectors as input to disentangle the appearance and geometric information from image or video sequences. For video data, a nonlinear transition model is introduced to both the appearance and geometric generators to capture the dynamics over time. The proposed scheme is general and can be easily integrated into different generative models. An extensive set of qualitative and quantitative experiments shows that the appearance and geometric information can be well disentangled, and the learned geometric generator can be conveniently transferred to other image datasets to facilitate knowledge transfer tasks.</p>
    </div>
  </div>
</div>
</li>


 <li><div class="row">
  <div class="col-sm-2 abbr">
    <abbr class="badge">Journal</abbr>
  </div>

  <div id="wu2019tale" class="col-sm-8">    
      <div class="title">A Tale of Three Probabilistic Families: Discriminative, Descriptive and Generative Models</div>
      <div class="author">
                Ying Nian Wu, <em>Ruiqi Gao</em>, Tian Han, and <a href="http://www.stat.ucla.edu/~sczhu/">Song-Chun Zhu</a>
      </div>
   <div class="periodical">    
        <em>Quarterly of Applied Mathematics (QAM)</em>, 2019
      </div>  

    <div class="links">
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
      <a href="https://arxiv.org/pdf/1810.04261.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
    </div>

    <!-- Hidden abstract block -->
    <div class="abstract hidden">
      <p>The pattern theory of Grenander is a mathematical framework where patterns are represented by probability models on random variables of algebraic structures. In this paper, we review three families of probability models, namely, the discriminative models, the descriptive models, and the generative models. A discriminative model is in the form of a classifier. It specifies the conditional probability of the class label given the input signal. A descriptive model specifies the probability distribution of the signal, based on an energy function defined on the signal. A generative model assumes that the signal is generated by some latent variables via a transformation. We shall review these models within a common framework and explore their connections. We shall also review the recent developments that take advantage of the high approximation capacities of deep neural networks.</p>
    </div>
  </div>
</div>
</li>
</ol>


<h2 class="year">2018</h2>
<ol class="bibliography">

 <li><div class="row">
  <div class="col-sm-2 abbr">
    <abbr class="badge">CVPR</abbr>
    <span class="award badge">Spotlight</span>
  </div>

  <div id="gao2018learning" class="col-sm-8">    
      <div class="title">Learning Energy­-Based Models as Generative ConvNets via Multi­-grid Modeling and Sampling</div>
      <div class="author">
                <em>Ruiqi Gao*</em>, Yang Lu*, Junpei Zhou, <a href="http://www.stat.ucla.edu/~sczhu/">Song-Chun Zhu</a>, and <a href="http://www.stat.ucla.edu/~ywu/" target="_blank">Ying Nian Wu</a>
        
      </div>
   <div class="periodical">    
        <em>Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2018
      </div>  

    <div class="links">
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
      <a href="https://arxiv.org/pdf/1709.08868.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      <a href="http://www.stat.ucla.edu/~ruiqigao/multigrid/main.html" class="btn btn-sm z-depth-0" role="button" target="_blank">Website</a>
      <a href="https://github.com/ruiqigao/Multigrid_learning" class="btn btn-sm z-depth-0" role="button" target="_blank">Code</a>
    </div>

    <!-- Hidden abstract block -->
    <div class="abstract hidden">
      <p>This paper proposes a multi-grid method for learning energy-based generative ConvNet models of images. For each grid, we learn an energy-based probabilistic model where the energy function is defined by a bottom-up convolutional neural network (ConvNet or CNN). Learning such a model requires generating synthesized examples from the model. Within each iteration of our learning algorithm, for each observed training image, we generate synthesized images at multiple grids by initializing the finite-step MCMC sampling from a minimal 1 × 1 version of the training image. The synthesized image at each subsequent grid is obtained by a finite-step MCMC initialized from the synthesized image generated at the previous coarser grid. After obtaining the synthesized examples, the parameters of the models at multiple grids are updated separately and simultaneously based on the differences between synthesized and observed examples. We show that this multi-grid method can learn realistic energy-based generative ConvNet models, and it outperforms the original contrastive divergence (CD) and persistent CD.
</p>
    </div>
  </div>
</div>
</li>

<li><div class="row">
  <div class="col-sm-2 abbr">
    <abbr class="badge">AAAI</abbr>
    <span class="award badge">Oral</span>
    <abbr class="badge">TPAMI</abbr>
  </div>

  <div id="xie2018cooperative" class="col-sm-8">    
      <div class="title">Cooperative Learning of Energy-Based Model and Latent Variable Model via MCMC Teaching</div>
      <div class="author">
                Jianwen Xie, Yang Lu, <em>Ruiqi Gao</em>, <a href="http://www.stat.ucla.edu/~sczhu/">Song-Chun Zhu</a>, and <a href="http://www.stat.ucla.edu/~ywu/" target="_blank">Ying Nian Wu</a>
        
      </div>
   <div class="periodical">    
        <em>The Thirty­-Second AAAI Conference on Artificial Intelligence (AAAI)</em>, 2018
      </div>  
      <div class="periodical">    
        <em>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</em>, 2019
      </div>  

    <div class="links">
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
      <a href="http://www.stat.ucla.edu/~ywu/CoopNets/doc/CoopNets_AAAI.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      <a href="http://www.stat.ucla.edu/~ywu/CoopNets/main.html" class="btn btn-sm z-depth-0" role="button" target="_blank">Website</a>
      <a href="https://github.com/ruiqigao/CoopNets" class="btn btn-sm z-depth-0" role="button" target="_blank">Code</a>
    </div>

    <!-- Hidden abstract block -->
    <div class="abstract hidden">
      <p>This paper proposes a cooperative learning algorithm to train both the undirected energy-based model and the directed latent variable model jointly. The learning algorithm interweaves the maximum likelihood algorithms for learning the two models, and each iteration consists of the following two steps: (1) Modified contrastive divergence for energy-based model: The learning of the energy-based model is based on the contrastive divergence, but the finite-step MCMC sampling of the model is initialized from the synthesized examples generated by the latent variable model instead of being initialized from the observed examples. (2) MCMC teaching of the latent variable model: The learning of the latent variable model is based on how the MCMC in (1) changes the initial synthesized examples generated by the latent variable model, where the latent variables that generate the initial synthesized examples are known so that the learning is essentially supervised. Our experiments show that the cooperative learning algorithm can learn realistic models of images.
</p>
    </div>
  </div>
</div>
</li>


<li><div class="row">
  <div class="col-sm-2 abbr">
    <abbr class="badge">CVPR</abbr>
    <span class="award badge">Oral</span>
  </div>

  <div id="xie2018learning" class="col-sm-8">    
      <div class="title">Learning Descriptor Networks for 3D Shape Synthesis and Analysis</div>
      <div class="author">
                Jianwen Xie*, Zilong Zheng*, <em>Ruiqi Gao</em>, Wenguan Wang, <a href="http://www.stat.ucla.edu/~sczhu/">Song-Chun Zhu</a>, and <a href="http://www.stat.ucla.edu/~ywu/" target="_blank">Ying Nian Wu</a>
        
      </div>
   <div class="periodical">    
        <em>Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2018
      </div>  

    <div class="links">
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
      <a href="https://openaccess.thecvf.com/content_cvpr_2018/papers/Xie_Learning_Descriptor_Networks_CVPR_2018_paper.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      <a href="http://www.stat.ucla.edu/~jxie/3DDescriptorNet/3DDescriptorNet.html" class="btn btn-sm z-depth-0" role="button" target="_blank">Website</a>
      <a href="https://github.com/jianwen-xie/3DDescriptorNet" class="btn btn-sm z-depth-0" role="button" target="_blank">Code</a>
    </div>

    <!-- Hidden abstract block -->
    <div class="abstract hidden">
      <p>This paper proposes a 3D shape descriptor network, which is a deep convolutional energy-based model, for modeling volumetric shape patterns. The maximum likelihood training of the model follows an “analysis by synthesis” scheme and can be interpreted as a mode seeking and mode shifting process. The model can synthesize 3D shape patterns by sampling from the probability distribution via MCMC such as Langevin dynamics. The model can be used to train a 3D generator network via MCMC teaching. The conditional version of the 3D shape descriptor net can be used for 3D object recovery and 3D object super-resolution. Experiments demonstrate that the proposed model can generate realistic 3D shape patterns and can be useful for 3D shape analysis.
</p>
    </div>
  </div>
</div>
</li>

</ol>


<h2 class="year">2017</h2>
<ol class="bibliography">

<li><div class="row">
  <div class="col-sm-2 abbr">
    <abbr class="badge">Journal</abbr>
  </div>

  <div id="lu2017exploring" class="col-sm-8">    
      <div class="title">Exploring Generative Perspective of Convolutional Neural Networks by Learning Random Field Models</div>
      <div class="author">
                Yang Lu, <em>Ruiqi Gao</em>, <a href="http://www.stat.ucla.edu/~sczhu/">Song-Chun Zhu</a>, and <a href="http://www.stat.ucla.edu/~ywu/" target="_blank">Ying Nian Wu</a>
        
      </div>
   <div class="periodical">    
        <em>Statistics and Its Interface</em>, 2017
      </div>  

    <div class="links">
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
      <a href="http://www.stat.ucla.edu/~sczhu/papers/Stat_interface_DeepFRAME_2018.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
    </div>

    <!-- Hidden abstract block -->
    <div class="abstract hidden">
      <p>This paper studies the convolutional neural network (ConvNet or CNN) from a statistical modeling perspective. The ConvNet has proven to be a very successful discriminative learning machine. In this paper, we explore the generative perspective of the ConvNet. We propose to learn Markov random field models called FRAME (Filters, Random field, And Maximum Entropy) models using the highly sophisticated filters pre-learned by the ConvNet on the big ImageNet dataset. We show that the learned models can generate realistic and rich object and texture patterns in natural scenes. We explain that each learned model corresponds to a new ConvNet unit at the layer above the layer of filters employed by the model. We further show that it is possible to learn a generative ConvNet model with a new layer of multiple filters, and the learning algorithm admits an EM interpretation with binary latent variables.
</p>
    </div>
  </div>
</div>
</li>


<li><div class="row">
  <div class="col-sm-2 abbr">
    <abbr class="badge">Journal</abbr>
  </div>

  <div id="gao2017correspondence" class="col-sm-8">    
      <div class="title">Correspondence of D. melanogaster and C. elegans developmental stages revealed by alternative splicing dynamics of conserved exon</div>
      <div class="author">
                <em>Ruiqi Gao</em>, and <a href="http://jsb.ucla.edu/about-jingyi-jessica-li" target="_blank">Jingyi Jessica Li</a>
        
      </div>
   <div class="periodical">    
        <em>BMC Genomics</em>, 2017
      </div>  

    <div class="links">
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
      <a href="https://bmcgenomics.biomedcentral.com/track/pdf/10.1186/s12864-017-3600-2?site=bmcgenomics.biomedcentral.com" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
    </div>

    <!-- Hidden abstract block -->
    <div class="abstract hidden">
      <p><b>Background:</b> We report a statistical study to find correspondence of D. melanogaster and C. elegans developmental stages based on alternative splicing (AS) characteristics of conserved cassette exons using modENCODE RNA-seq data. We identify “stage-associated exons” to capture the AS characteristics of each stage and use these exons to map pairwise stages within and between the two species by an overlap test. <br>
<b>Results:</b> Within fly and worm, adjacent developmental stages are mapped to each other, i.e., a strong diagonal pattern is observed as expected, supporting the validity of our approach. Between fly and worm, two parallel mapping patterns are observed between fly early embryos to early larvae and worm life cycle, and between fly late larvae to adults and worm late embryos to adults. We also apply this approach to compare tissues and cells from fly and worm. Findings include the high similarity between fly/worm adults and fly/worm embryos, groupings of fly cell lines, and strong mappings of fly head tissues to worm late embryos and male adults. Gene ontology and KEGG enrichment analyses provide a detailed functional annotation of the identified stage-associated exons, as well as a functional explanation of the observed correspondence map between fly and worm developmental stages. <br>
<b>Conclusions:</b> Our results suggest that AS dynamics of the exon pairs that share similar DNA sequences are informative for finding transcriptomic similarity of biological samples. Our study is innovative in two aspects. First, to our knowledge, our study is the first comprehensive study of AS events in fly and worm developmental stages, tissues, and cells. AS events provide an alternative perspective of transcriptome dynamics, compared to gene expression events. Second, our results do not entirely rely on the information of orthologous genes. Interesting results are also observed for fly and worm cassette exon pairs with DNA sequence similarity but not in orthologous gene pairs.
</p>
    </div>
  </div>
</div>
</li>

</ol>

</div>

  </article>

</div>

    </div>

    <!-- Footer -->

    

  </body>

  <!-- jQuery -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>

  <!-- Bootsrap & MDB scripts -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/2.4.4/umd/popper.min.js" integrity="sha512-eUQ9hGdLjBjY3F41CScH3UX+4JDSI9zXeroz7hJ+RteoCaY+GP/LDoM8AO+Pt+DRFw3nXqsjh9Zsts8hnYv8/A==" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/js/bootstrap.min.js" integrity="sha512-M5KW3ztuIICmVIhjSqXe01oV2bpe248gOxqmlcYrEzAvws7Pw3z6BK0iGbrwvdrUQUhi3eXgtxp5I8PDo9YfjQ==" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/js/mdb.min.js" integrity="sha512-Mug9KHKmroQFMLm93zGrjhibM2z2Obg9l6qFG2qKjXEXkMp/VDkI4uju9m4QKPjWSwQ6O2qzZEnJDEeCw0Blcw==" crossorigin="anonymous"></script>

  
<!-- Mansory & imagesLoaded -->
<script defer src="https://unpkg.com/masonry-layout@4/dist/masonry.pkgd.min.js"></script>
<script defer src="https://unpkg.com/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
<script defer src="./js/mansory.js" type="text/javascript"></script>


  


<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-180825462-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag() { dataLayer.push(arguments); }
  gtag('js', new Date());

  gtag('config', 'UA-180825462-1');
</script>


<!-- Load Common JS -->
<script src="./js/common.js"></script>

<!-- Load DarkMode JS -->
<script src="./js/dark_mode.js"></script>


</html>
